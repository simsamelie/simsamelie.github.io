---
description: "Expanding my knowledge by building a model from scratch, then using Pytorch to simplify, improve, and eventually submit my predictions into a Kaggle competition!"
---

# Stochastic Gradient Descent
> "Expanding my knowledge by building a model from scratch, then using Pytorch to simplify, improve, and eventually submit my predictions into a Kaggle competition!"
 
## Building a model from scratch

After trusting the process with my initial model in image classification, I was very eager for this next chapter. Chapter 3 of the fastai course delves more into the inner workings of a machine learning model, using Stochastic Gradient Descent as its running example. The course features the Titanic dataset, using functions in Excel to demonstrate an updating linear regression model. I wanted to take this further by pursuing my first model _from scratch_ in Python, to eventually submit into the Kaggle Titanic competition!

# Creating the initial model

Before I began to think about the models actual architecture, I wanted to clean the data. I decided to use pandas, since I was already familiar with the package, to remove the redundant columns like ticket code and cabin number, convert the class number and location of embarking into two boolean columns, and normalise the age data. For the 'Fare' column, I applied a log scale, as I understand is common for financial data due to the distribution of finances throughout the population. I then used the ```data.sample``` operator to split the data into training and validation, and set aside the 'Survived' data for testing. At this point, I also added a column of ones to account for the bias parameter, or the y-intercept of our linear equation, $y = Ax + c$. Although, this became redundant later in the process due to Pytorch functions including a seperate parameter class for the bias.

To begin the construction of the model, I initialised two functions: the matrix multiplication- the core of the prediction- and the mean-squared error- the loss function. I also generated 10 random numbers to act as our parameters: weights for each data class ('Age', 'Gender' etc.) and one bias.

    def f(t, params):
    params = tensor(params)
    return params@t

    def mse(pred, target):
    return ((pred-target)**2).mean()

    para = torch.randn(10, requires_grad= True)

Using these functions, we can calculate both the predictions using the models initial parameters and the gradient of each parameter. The predictions of survival are combined with the _actual_ survival data to create the loss, the value that quantifies how accurate the models prediction was. Calculating the gradients is as simple as one line of code, ```loss.backward()```, which performs a backpropogation.

    preds = f(train_set, para)
    loss = mse(preds,survived)
    loss.backward()

We can then use these gradients to update our parameters and make our model (theoretically) more accurate!

    para.data -= para.grad.data*lr
    para.grad = None

Here, ```lr``` is the _learning rate_ which simply scales the calculated gradient value so our model isn't bouncing around unrealistic values during the training period. We can then calculate the loss again, as above. This is the fundamental process of a Stocastic Gradient Descent model. For the accuracy of the model, we will use the validation set. As before in the image classifier, it is important that the model behaves well on the validation set along with the trianing set. If it does not, the model can be 'overfitting' or learning the training data by heart and thus does not perform well on data it hasn't seen before, ie. the validation set. To analyse accuracy, we scale the predictions between 0 and 1 using a Sigmoid function, since our actual data values are given in boolean form. That is 

$$ f(x) = \frac{1}{1+e^{-x}} $$

<img width="500" height="334" alt="Logistic-curve svg" src="https://github.com/user-attachments/assets/39f1941c-765a-44c0-9c4f-f53e887b24c2" />

We can then say if a prediction is $> 0.5$, we have predicted survival, and $\leq 0.5$ we have predicted death.

    corrects = (sigmoid(preds)>0.5).float() == survived
    corrects.float().mean().item()

The final line returns the mean of the correct predictions, or the total accuracy for the training set. 

Now, we have everything we need to simplify into functions and train our first model. 

    params = torch.randn(10, requires_grad= True)

    def linear(x,params):
    params = tensor(params)
    return params@x

    def calc_grad(x,y,model,params):
    preds = model(x,params)
    loss = mse(preds, y)
    loss.backward()

We can simplify the training stage into a function:
    
    def train_epoch(x,y,model,lr,params):
    calc_grad(x,y,model,params)
    for p in params:
      params.data -= params.grad.data*lr
      params.grad = None

Then define an accuracy function

    def acc(x,y):
    preds = x.sigmoid()
    correct = (preds>0.5) == y
    return correct.float().mean()

Using a learning rate of $0.001$, I trained this model over 100 epochs. 

    for i in range(100):
      train_epoch(train_set, survived, linear, lr, params)
      print(acc(linear(valid_set, params), survived_test))

This initial model returned a validation accuracy of $\approx 50$%. This certainly isn't great, so I moved on to streamlining the model.

# Improving the model
- Simplifying the code, now that I understand it, using Pytorch (simplifying the functions, using SGD optimiser and varying the learning rate)
- Fiddling with batch sizes, learning rates, types of scheduler etc. to fix the bouncing
- Normalising all of the data, instead of picking and choosing, so everything was either a boolean or between 0 and 1, improved the accuracy from 60% to 75%.

Introducing the ReLU
- What it does etc. , getting a similar accuracy of 72%. Why is the ReLU not more accurate than the linear model?

Conclusion
- Happy with the 75% accuracy and feel like I understand how to the model works and is trained. However, I would like to aim for a higher accuracy, maybe increasing by another 10% to 85% accuracy. Also, can I use this same model architecture to predict weather events or temperature?
