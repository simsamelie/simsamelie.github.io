---
layout: post
title: "Stochastic Gradient Descent"
excerpt: "Expanding my knowledge by building a model from scratch, then using Pytorch to simplify, improve, and eventually submit my predictions into a Kaggle competition!"
---
# Stochastic Gradient Descent
> Expanding my knowledge by building a model from scratch, then using Pytorch to simplify, improve, and eventually submit my predictions into a Kaggle competition!

## Building a model from scratch

After trusting the process with my initial model in image classification, I was very eager for this next chapter. Chapter 3 of the fastai course delves more into the inner workings of a machine learning model, using Stochastic Gradient Descent as its running example. The course features the Titanic dataset, using functions in Excel to demonstrate an updating linear regression model. I wanted to take this further by pursuing my first model _from scratch_ in Python, to eventually submit into the Kaggle Titanic competition!

# Creating the initial model

Before I began to think about the models actual architecture, I wanted to clean the data. I decided to use pandas, since I was already familiar with the package, to remove the redundant columns like ticket code and cabin number, convert the class number and location of embarking into two boolean columns, and normalise the age data. For the 'Fare' column, I applied a log scale, as I understand is common for financial data due to the distribution of finances throughout the population. I then used the ```data.sample``` operator to split the data into training and validation, and set aside the 'Survived' data for testing. At this point, I also added a column of ones to account for the bias parameter, or the y-intercept of our linear equation, $y = Ax + c$. Although, this became redundant later in the process due to Pytorch functions including a seperate parameter class for the bias.

To begin the construction of the model, I initialised two functions: the matrix multiplication- the core of the prediction- and the mean-squared error- the loss function. I also generated 10 random numbers to act as our parameters: weights for each data class ('Age', 'Gender' etc.) and one bias.

    def f(t, params):
    params = tensor(params)
    return params@t

    def mse(pred, target):
    return ((pred-target)**2).mean()

    para = torch.randn(10, requires_grad= True)

Using these functions, we can calculate both the predictions using the models initial parameters and the gradient of each parameter. The predictions of survival are combined with the _actual_ survival data to create the loss, the value that quantifies how accurate the models prediction was. Calculating the gradients is as simple as one line of code, ```loss.backward()```, which performs a backpropogation.

    preds = f(train_set, para)
    loss = mse(preds,survived)
    loss.backward()

We can then use these gradients to update our parameters and make our model (theoretically) more accurate!

    para.data -= para.grad.data*lr
    para.grad = None

Here, ```lr``` is the _learning rate_ which simply scales the calculated gradient value so our model isn't bouncing around unrealistic values during the training period. We can then calculate the loss again, as above. This is the fundamental process of a Stocastic Gradient Descent model. For the accuracy of the model, we will use the validation set. As before in the image classifier, it is important that the model behaves well on the validation set along with the trianing set. If it does not, the model can be 'overfitting' or learning the training data by heart and thus does not perform well on data it hasn't seen before, ie. the validation set. To analyse accuracy, we scale the predictions between 0 and 1 using a Sigmoid function, since our actual data values are given in boolean form. That is 

$$ f(x) = \frac{1}{1+e^{-x}} $$

<img width="500" height="334" alt="Logistic-curve svg" src="https://github.com/user-attachments/assets/39f1941c-765a-44c0-9c4f-f53e887b24c2" />

We can then say if a prediction is $> 0.5$, we have predicted survival, and $\leq 0.5$ we have predicted death.

    corrects = (sigmoid(preds)>0.5).float() == survived
    corrects.float().mean().item()

The final line returns the mean of the correct predictions, or the total accuracy for the training set. 

Now, we have everything we need to simplify into functions and train our first model. 

    params = torch.randn(10, requires_grad= True)

    def linear(x,params):
    params = tensor(params)
    return params@x

    def calc_grad(x,y,model,params):
    preds = model(x,params)
    loss = mse(preds, y)
    loss.backward()

We can simplify the training stage into a function:
    
    def train_epoch(x,y,model,lr,params):
    calc_grad(x,y,model,params)
    for p in params:
      params.data -= params.grad.data*lr
      params.grad = None

Then define an accuracy function

    def acc(x,y):
    preds = x.sigmoid()
    correct = (preds>0.5) == y
    return correct.float().mean()

Using a learning rate of $0.001$, I trained this model over 100 epochs. 

    for i in range(100):
      train_epoch(train_set, survived, linear, lr, params)
      print(acc(linear(valid_set, params), survived_test))

This initial model returned a validation accuracy of $\approx 50$%. This certainly isn't great, so I moved on to streamlining the model.

# Improving the model
Now that I felt comfortable with the inner workings of the model, I used Pytorch to simplify and improve the model. To begin using Pytorch, I had to convert my pandas dataset into a Pytorch dataframe using a _HousingDataset_ class as below.

    class HousingDataset(Dataset):
    def __init__(self, features, targets):
        self.X = torch.tensor(features, dtype=torch.float32)
        self.y = torch.tensor(targets.values, dtype=torch.float32).view(-1, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

I could then construct a ```DataLoader```, as in my Image Classifier, to then use a process called 'minibatching' in which the gradients are calculated based on a 'batch' of the training data and not each individual piece. The idea is that then the model becomes more accurate since it is viewing the data as a collection, rather than individually, and it can start to see patterns that lend to the result.

    train_set = HousingDataset(data.values, survived)
    valid_set = HousingDataset(valid.values, survived_test)
    test_set = tensor(test.values)

    train_dl = DataLoader(train_set, batch_size = 64, shuffle=True)
    valid_dl = DataLoader(valid_set, batch_size = 64, shuffle=True)

I replaced the randomisation of the parameters and the definition of the linear function with a simple function from Pytorch, ```nn.Linear```, and the updating of the gradients simplified nicely into the Pytorch optimiser, ```optim.SGD```

    linear_model = nn.Linear(9,1)
    opt = optim.SGD(linear_model.parameters(),lr = 1e-3)

Since Pytorch handled them for me, I could remove the 'params' arguement from my functions and thus define my final Stochastic Gradient Descent model as follows:

    def calc_grad(xb,yb,model):
    preds = model(xb)
    loss = mse(preds, yb)
    loss.backward()

    def train_epoch(model):
      for xb,yb in train_dl:
        calc_grad(xb,yb,model)
        opt.step()
        opt.grad = None

    def acc(x,y):
       preds = x.sigmoid()
       correct = (preds>0.5) == y
       return correct.float().mean()

    def validate(model):
      accuracy = [acc(model(x),y) for x,y in valid_dl]
      return round(torch.stack(accuracy).mean().item(), 4)

    def train_acc(model):
      accuracy = [acc(model(x),y) for x,y in train_dl]
      return round(torch.stack(accuracy).mean().item(), 4)

    def train_model(model, epochs):
        v_acc = []
        t_acc = []
        for i in range(epochs):
          train_epoch(model)
          v_acc += [validate(model)]
          t_acc += [train_acc(model)]
        return [v_acc,t_acc]

When training this model for 100 epochs again, I faced a very interesting problem. The model seemed to be 'overfitting' in such a way that it bounced between a very low and a very high validation accuracy.

<img width="547" height="418" alt="originalpoopacc" src="https://github.com/user-attachments/assets/19c1f002-c5cb-433d-9336-1be5a424dad1" />

To fix this overfitting, editting the hyperparameters was definitely necessary. I experimented with different learning rates, batch sizes and even experimented with some learning rate 'schedulers' which updated the learning rate at epoch intervals. I found that a batch size of 64 was the most effective and a learning rate of 0.001 stopped these accuracy jumps whilst still being large enough to make sufficient changes to the gradient and actually increase the accuracy before plateauing. Here, I achieved an accuracy of approximately 60%. Still, unsatisfied, I continued. 

At the start, I had normalised only certain columns of the data- those that had the highest or most varied entries. However, this then lead to the columns such as 'SibSp' (the number of sibling or spouses aboard) having far too much influence. I decided to normalise all of the data so that each entry was between 0 and 1. As I had expected, this improved the data massively, and I achieved a validation accuracy of 80%!

<img width="567" height="438" alt="Untitled" src="https://github.com/user-attachments/assets/467fb5d6-3c0b-47b1-ab02-cbc36aff74b4" />

Submitting my models predictions of the test set into the Kaggle competition, I achieved a 75% accuracy which was significant enough for me to consider my first SGD model a success.

# Introducing the ReLU
To push it that bit further, I worked to introduce a ReLU (Rectified Linear Unit) function into my model to turn it into a true neural network. Using multiple layers of linear models, we can slot in a ReLU in between them. This function preserves positive outputs and replaces negative outputs with a 0, so our linear model then looks something like this.

<img width="1280" height="960" alt="Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs" src="https://github.com/user-attachments/assets/a9ad683e-4fac-4d66-96f1-d5b5816d9d73" />

With these functions seperating our linear layers, we actually can produce a non-linear function to represent our model and thus allow for more complexity and tailored predictions. I formed my neural net by again utilizing Pytorch functions, both ```nn.ReLU``` and ```nn.Sequential``` to perform these operators _sequentially_.

    simple_net = nn.Sequential(
    nn.Linear(9,10),
    nn.ReLU(),
    nn.Linear(10,1)
    )

This model did behave similarly to my linear model, yet the validation accuracy seemed to climb slower but peak higher!

<img width="567" height="438" alt="after normalising data RELU model" src="https://github.com/user-attachments/assets/1e12b0c7-f369-4900-9fc7-36da9b10499d" />

However, when submitting again my predictions into the Kaggle competition, I only achieved an accuracy of 72%. 

# Conclusion
I'd imagined the neural net would be more accurate than the linear so I'd like to expand on my model in the future and include more linear layers in an attempt to raise this accuracy. Perhaps the data is just more suited to a linear approach. Regardless, I'm very happy with my models accuracy, despite it not being absolutely perfect. I feel now that I understand fully how a machine learning model operates and am more confident that I could construct one efficiently given a dataset. However, I would like to aim for a higher accuracy in the future, maybe increasing by another 10% to 85% accuracy. Moving forward, I'm interested to see what other problems this model architecture could be used to tackle. My immediate idea would be to predict future weather events, temperature or UV level based on current weather trends.
